---
title: "notebook20181207"
author: "Mark Hagemann"
date: "December 7, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I now have the idea to try and easily calculate the Hessian of the log likelihood function. This will allow me to get the eigenvectors and eigenvalues, teasing out the non-identifiable parameters, and the relative level of identifiability for the other parameters. 

```{r}
foo <- reachdata$Ganges %>% 
  swot_sset(keeplocs = 1:3, keeptimes = 1:10 * 10) %>% 
  swot_bamdata()

foofun <- bam_errmatfun(foo)

foofun(A0 = exp(bamr:::estimate_logA0(foo$Wobs)), 
       Q = rep(500, 10), logn = -3.5)

bamr:::compose_bam_inputs(foo)
```


Need to better articulate the goal and see whether any of the stuff I've written can help me. 

Given likelihood:

$$
\ell(n, \bar{q}, \dot{q_t}, A_{0,s}, \sigma; W_{st}, S_{st}, \delta A_{st})
$$

1. Maximize $\ell$, obtain parameters that do this. 
2. Calculate gradient, hessian of parameters
3. Find eigenvectors

Seems I need a likelihood function, then I can (hopefully automatically) differentiate.

```{r}
llfun1 <- reachdata$Ganges %>% 
  swot_bamdata() %>% 
  bam_llfun_vector(negative = TRUE)



```

See if I can optimize (get maximum likelihood). Get starting vals by cheating.

```{r}
bestparams <- calcManParams(reachdata$Ganges)

bestparams$sigma
names(bestparams)

bestparamvec <- with(bestparams, c(A0, log(n), Q, sigma))

bestparamvec

foo <- nlm(llfun1, p = bestparamvec)
```

This might not converge easily. See what I can do with numDeriv.

```{r}
library(numDeriv)

hess1 <- numDeriv::hessian(llfun1, bestparamvec)
eig1 <- eigen(hess1)

eig1$values
eig1$vectors[, 1] %>% plot()
eig1$vectors[, 2] %>% plot()
eig1$vectors[, 3] %>% plot()
eig1$vectors[, 4] %>% plot()
eig1$vectors[, 5] %>% plot()
```


This is proving harder to interpret than I'd hoped. Part of the issue may be scale of the parameters. 

```{r}
plot(diag(hess1), log = "y")
```

Also I should add in the qbar parameter. 

```{r}
llfun2 <- reachdata$Ganges %>% 
  swot_bamdata() %>% 
  bam2_llfun_vector(negative = TRUE) # This is the new one.

names(bestparams)
bestparamvec2 <- with(bestparams, c(log(A0), log(n), logQbar, sigma, logQdot))

llfun2(bestparamvec2)

# hess2 <- hessian(llfun2, x = bestparamvec2)

ns <- nrow(reachdata$Ganges$W)

hess2_part <- hess2[1:(ns + 3), 1:(ns + 3)]

eig2 <- eigen(hess2_part)

eig2$values
plot(eig2$values)
plot(eig2$vectors[, 1])
plot(eig2$vectors[, 2])
plot(eig2$vectors[, 3])
plot(eig2$vectors[, 4])
plot(eig2$vectors[, 8])

plot(eig2$vectors[, ns + 3])
```

What about A0 inference only?

```{r}
hess2_A0part <- hess2[c(1:ns, ns + 3), c(1:ns, ns + 3)]

eig2_A0 <- eigen(hess2_A0part)

eig2_A0$values
plot(eig2_A0$values)
plot(eig2_A0$vectors[, 1])
plot(eig2_A0$vectors[, 2])

min(eig2_A0$values) / max(eig2_A0$values)

```

Next I'll try a hierarchical parameter on logA0. 

```{r}
llfun3 <- reachdata$Ganges %>% 
  swot_bamdata() %>% 
  bam3_llfun_vector(negative = TRUE) # This is the new one.

names(bestparams)
bestparamvec3 <- with(bestparams, c(mean(log(A0)), log(n), 
                                    logQbar, sigma, 
                                    log(A0) - mean(log(A0)), logQdot))

llfun3(bestparamvec3)

hess3 <- hessian(llfun2, x = bestparamvec2)
```

Last 3 eigenvalues must be zero (logn - logqbar tradeoff, then 2 sum-to-zero constraints). So I can lop these off and focus on the higer-order parameters? Just logqbar, logA0bar, logn, sigma. Next iteration I should hold logn fixed and just look at logqbar, logA0bar, sigma tradeoff. 

```{r}

eig3 <- eigen(hess3)
eig3_ss <- eigen(hess3[1:4, 1:4])

eig3$values
eig3_ss$values

library(Matrix)
rankMatrix(hess3, tol = 1e-5)

max(eig3_ss$values) / min(eig3_ss$values)

```

Last 3 eigenvalues are not zero! One is negative, meaning there is an axis along which curvature is negative, and nll is concave!

```{r}
plot(eig3$vectors[, 374])
plot(eig3$vectors[1:10, 374])
```

I still need to do the damn optimization to get the MLE, which is where I should be estimating the information. Problem is this is apparently non-convex. 

Is it possible at all to write without qdot?

*If* I can assume model error to be negligible, then I can write:

$$
\dot{q}_{t} \approx \bar{x}_{\cdot t} - \bar{q} - \tilde{n} + \frac{5}{3 n_s}\sum_{s = 1}^{n_s} \log (A_{0,s} + \delta A_{st}) 
$$

Some parameters then cancel, and I get:

$$
x_{st} - \bar{x}_{\cdot t} \sim N(-\frac{5}{3}(a_{st} - \bar{a}_{\cdot t}), \sigma)
$$

where $a_{st} = \log (A_{0,s} + \delta A_{st})$

Can I rewrite the approximation as an equality? Sure! For completeness, but I can probably approximate just fine:

$$
\dot{q}_{t} = \bar{x}_{\cdot t} - \bar{q} - \tilde{n} + \frac{5}{3 n_s}\sum_{s = 1}^{n_s} \log (A_{0,s} + \delta A_{st}) - \bar{\epsilon}_{\cdot t} 
$$
where $\epsilon_{st}$ is the original error term with sd $\sigma$. Now can I write this likelihood for the data?

Let $y_{st} = x_{st} - \bar{x}_{\cdot t}$. Then

$$
y_{st} \sim - \frac{5}{3}(\log (A_{0,s} + \delta A_{st}) - \frac{1}{n_s} \sum_{s = 1}^{n_s}{\log (A_{0,s} + \delta A_{st})}, \sigma_*)
$$

where $\sigma_*$ is the standard deviation of $\epsilon_{st} - \bar{\epsilon}_{\cdot t}$ A decent approximation should be:

$$
\sigma_*^2 \approx \frac{n_s + 1}{n_s} \sigma^2
$$

although this will likely be off for small $n_s$. NEED TO CHECK AT SOME POINT!

This should be sufficient to implement. 

$$
\begin{aligned}
\ell(A_{0,s}, \sigma_*; y_{st}, \delta A_{st}) &= \log(f(y_{st}, a_{st} | A_{0,s}, \sigma_*)) - \log{(A_{0,s} + \delta A_{st}}) \\ 
&= - \frac{n_t}{2} \log(2\pi \sigma_*^2) - \frac{1}{2 \sigma_*^2}
\end{aligned}
$$

Needs more paper work. 

$$
\begin{aligned}
\ell(A_{0,s}, \sigma_*; y_{st}, \delta A_{st}) &= \log(f(y_{st}, a_{st} | A_{0,s}, \sigma_*)) - \log{(A_{0,s} + \delta A_{st}}) \\ 
&= - \frac{n_s n_t}{2} \log(2\pi \sigma_*^2) - \frac{1}{2 \sigma_*^2} \sum_{st}(y_{st} + \frac{5}{3}(a_{st} - \bar{a}_{\cdot t}))^2 - \sum_{st} a_{st}
\end{aligned}
$$

Derivatives:

$$
\begin{aligned}
\frac{\partial \ell}{\partial A_{0,s}} &= -\frac{5}{3 \sigma_*^2} (y_{st} + )
\end{aligned}
$$

Hold on. This is shaping up to have zero gradient for A0. Can I verify numerically/empirically?

```{r}
newllfun <- function(bamdata, negative = FALSE, gradient = FALSE) {
  ns <- nrow(bamdata$Wobs)
  nt <- ncol(bamdata$Wobs)
  
  xmat <- 1/2 * log(bamdata$Sobs) - 2/3 * log(bamdata$Wobs)
  xbarvec <- apply(xmat, 2, mean)
  ymat <- xmat - swot_vec2mat(xbarvec, xmat)
  
  
  llfun <- function(params) {
    stopifnot(length(params) == (ns + 1))
    
    sigma <- params[ns + 1]
    A0vec <- params[1:ns]
    Amat <- bamdata$dAobs + swot_vec2mat(A0vec, ymat)
    logAmat <- log(Amat)
    logAbarvec <- apply(logAmat, 2, mean)
    
    logAmat_ctr <- logAmat - swot_vec2mat(logAbarvec, ymat)
    
    mumat <- ymat + 5/3 * logAmat_ctr
    ldens1 <- dnorm(as.vector(mumat), mean = 0, sd = sigma, log = TRUE)
    ldensvec <- ldens1 - as.vector(logAmat) # Jacobian adjustment
    llik <- sum(ldensvec)

    Mmat <- 10 / 3 * (ymat + 5/3 * logAmat_ctr)
    Vmat <- -1 / (ns * Amat)
    Pmat <- -ns * Mmat * Vmat
    
    bit1 <- as.vector(rep(1, ns) %*% Mmat %*% t(Vmat))
    bit2 <- apply(Pmat, 1, sum)
    
    A0grad1 <- -1 / (2 * sigma^2) * (bit1 + bit2)
    A0jacadj <- apply(- 1 / Amat, 1, sum) # derivative of log Jacobian
    A0grad <- A0grad1 + A0jacadj

    sigmagrad1 <- -ns * nt / (sigma)
    # sigmagrad1 <- 0 # because I transformed already and use z score. 
    sigmagrad2 <- sum(mumat^2) / sigma^3
    grad <- c(A0grad, sigmagrad1 + sigmagrad2)

    if (negative) {
      llik <- -llik
      grad <- -grad
    }

    if (gradient) {
      attr(llik, "gradient") <- grad
    }
    
    llik
  }
  
  llfun
}

```

Test it out. Is the gradient zero?

```{r}
newll1 <- reachdata$Ganges %>% 
  swot_bamdata() %>% 
  newllfun(gradient = TRUE)

names(bestparams)
newparams <- with(bestparams, c(A0, sigma))
newparams

newll1(params = newparams)

newllderiv <- grad(newll1, x = newparams)

newllderiv
attr(newll1(params = newparams), "gradient")
plot(newllderiv / attr(newll1(params = newparams), "gradient"))
plot(attr(newll1(params = newparams), "gradient") / newllderiv)
```

Nope. Time for more math. I think I just forgot about the Jacobian? The Jacobian contribution to the gradient is 

$$
\frac{\partial}{\partial A_{0,s}} \sum_{s',t} \log(A_{0,s'} + \delta A_{s't}) = 
\sum_{t = 1}^{n_t} \frac{1}{A_{0,s} + \delta A_{st}}
$$

That's not it either. Apparently. More math!

Here's what I have now:

$$
\begin{aligned}
\frac{\partial }{\partial A_{0,s}} \sum_{s', t} (y_{s't} + \frac{5}{3} (a_{s't} - \bar{a}_{\cdot t}))^2 &= \mathbf{1'MV' + P1}

\end{aligned}
$$

where the components of $\mathbf{M}$, $\mathbf{V}$ and $\mathbf{P}$ are

$$
m_{st} = \frac{10}{3}(y_{st} + \frac{5}{3}(a_{st} - \bar{a}_{\cdot t})) \\
v_{st} = \frac{-1}{n_s (A_{0,s} + \delta A{st})} \\
p_{st} = m_{st} / (A_{0,s} + \delta A_{st})
$$

Try this on. 

It's working!! Mostly. sigma component of gradient is a little bit off; numerical version fits better (perfectly) to analytical without -n/sigma (first) term. Investigate. 

```{r}
foofun <- function(x) {
  
  llfun <- function(params) {
    mu <- params[1]
    sigma <- params[2]
    z <- (x - mu) / sigma
    llik <- sum(dnorm(x, mean = mu, sd = sigma, log = TRUE))
    
    n <- length(x)
    dmu <- - 2 * n * (mean(x) - mu) / (2 * sigma^2)
    dsigma1 <- -n / sigma
    # dsigma1 <- 0
    dsigma2 <- sum(z^2) / sigma
    grad <- c(dmu, dsigma1 + dsigma2)
    
    attr(llik, "gradient") <- grad
    llik
  }
}

foo <- foofun(rnorm(10, 3, 2))
foo(c(1, 1))

foo(c(2, 1.8))



grad(foo, c(2, 1.8))
```

Well isn't that funny! It matters whether you transform first or not. Makes sense when you think about it. 

Anyway, this is now working. This notebook is now almost a week old, but it accomplished a lot. Recap:

- prototyped workflow for calculating Hessian of log-likelihood, inspecting eigendecomposition, and interpreting
    - Showed lack of information in [logn, logqbar] direction.
    - Showed concavity of likelihood in bam case (indefinite info matrix)
- Wrote new BAM-based likelihood function eliminating q, n. 
    - calculated (finally!) gradient, confirmed numerically. 
    
Next steps:

- put new likelihood function into lib/ DONE.
- get MLE for A0 on Pepsi 1 cases
- calculate observed information matrix for new likelihood on Pepsi 1
- Show eigenvalues, first and last eigenvectors
- Connect to accuracy of MLE A0 estimate.
- Document math in reports/ rmarkdown document