---
title: "notebook20180806"
author: "Mark Hagemann"
date: "August 6, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Today I'm tracking down the discrpencies between gibbs results and BAM results. 

## Normal-normal model, known variance

First, compare normal-normal posterior for my gibbs vs. stan.

```{stan output.var = "testcase"}
data {
  vector[10] x;
  real muhat;
  real sigsq;
  real musd;
}

parameters {
  real mu;
}

model {
  x ~ normal(mu, sqrt(sigsq));
  mu ~ normal(muhat, musd);
}

```

```{r}
library(rstan)

testdata <- list(x = rnorm(10, 10, 1),
                 muhat = 3,
                 musd = 1,
                 sigsq = 1)
post_stan <- sampling(testcase, data = testdata, cores = 3)

stan_trace(post_stan)

```

```{r}
mu_post <- postmu(primu = testdata$muhat, prisigsq = testdata$musd^2, 
                 likmu = mean(testdata$x), liksigsq = testdata$sigsq, n = 10)
ssq_post <- postsigsq(prisigsq = testdata$musd^2, liksigsq = testdata$sigsq, n = 10)

plot(rnorm(1000, mu_post, sqrt(ssq_post)), type = "l")
```

That much is good. Now try a prior on sigsq.

## Normal-gamma model


```{stan output.var = "testcase2"}
data {
  int nx;
  vector[nx] x;
  real muhat;
  //real sigsq;
  real musd;
  real alpha;
  real beta;
}

parameters {
  real mu;
  real prec;
}

transformed parameters {
  real sigma;
  sigma = sqrt(1 / prec);
}

model {
  x ~ normal(mu, sigma);
  mu ~ normal(muhat, musd);
  prec ~ gamma(alpha, beta);
}
```

```{r}
invgam_mom(mean = 1, sd = 10)
hist(1 / sqrt(rgamma(10000, shape = 2.01, rate = 1.01)))

nx <- 30
testdata <- list(nx = nx, 
                 x = rnorm(nx, 10, 1),
                 muhat = 3,
                 musd = 10,
                 # sigsq = 1,
                 alpha = 2.01,
                 beta = 1.01)
post_stan <- sampling(testcase2, data = testdata, cores = 3,
                      control = list(adapt_delta = 0.99))
```

```{r}
pairs(post_stan)
stan_trace(post_stan, inc_warmup = TRUE)
stan_trace(post_stan, inc_warmup = FALSE)
stan_trace(post_stan, inc_warmup = FALSE, pars = "sigma")

stan_hist(post_stan)
summary(post_stan)$summary

```

```{r}
niter <- 10000

inits <- list(
  mu = 0, 
  prec = 0.01
)

precchain <- sigmachain <- muchain <- numeric(niter)

state <- inits
for (i in 1:niter) {
  cat(".")
  if (i %% 100 == 0) 
    cat(i, "\n")
  state$mu <- sample_normal(priormu = testdata$muhat, priorsigsq = (testdata$musd)^2, 
                            likmu = mean(testdata$x), liksigsq = 1 / state$prec, 
                            likn = testdata$nx)
  muchain[i] <- state$mu
  state$prec <- rgamma(1, shape = postalpha(prialpha = testdata$alpha, n = testdata$nx),
                         rate = postbeta(pribeta = testdata$beta, y = testdata$x, state$mu))
  precchain[i] <- state$prec
  sigmachain[i] <- 1 / sqrt(state$prec)
}

plot(muchain[-1:-5000], type = "l")
plot(sigmachain[-1:-5000], type = "l")

hist(sigmachain[-1:-5000])
hist(precchain[-1:-5000])

summary(post_stan)$summary
summary(data.frame(mu = muchain[-1:-5000], 
                   prec = precchain[-1:-5000], 
                   sigma = sigmachain[-1:-5000]))
```

OK, those agree very nicely. Conclusion: Any discrepancies between stan and gibbs vis a vis mcfli are not from my conception of normal-gamma. Instead, they must be either:

- bug in sampling statements, or
- misstated A0 sampler

The first is easy enough to look for (but may be hard to find). The second would mean that my hand-wavey aggregation of individual A0 estimates is insufficient and I may need to take a metropolis-hastings-in-gibbs approach. Which would be harder. 

## Truncated lognormal-normal-gamma model

Now I'll add the A0 and dA terms to the model. But this also requires adding a dimension to the data (akin to space). I'll just use 2 "locations".

The simplest model of this kind I can think of is:

$$
A_{0,s} + \delta A_{st} \sim logN(\mu_s, \sigma_i) \\
\mu_s \sim N(\hat{\mu_s}, \sigma_\mu) \\
\sigma^2_s \sim IG(\alpha, \beta)
$$

```{stan output.var = "testcase3"}
data {
  int ns;
  int nt;
  //vector[nx] x[2];
  vector[nt] dA[ns];
  vector[ns] muhat;
  //real sigsq;
  real musd;
  real alpha;
  real beta;
}

parameters {
  vector[ns] mu;
  vector<lower=0>[ns] A0;
  vector<lower=0.01>[ns] prec;
}

transformed parameters {
  //vector[nx] A[2]; 
  vector<lower=0>[ns] sigma;
  
  sigma = sqrt(1 ./ prec);
}

model {
  // x ~ normal(mu, sigma);
  
  for (i in 1:ns) {
    A0[i] + dA[i] ~ lognormal(mu[i], sigma[i]);
  }
  
  mu ~ normal(muhat, musd);
  prec ~ gamma(alpha, beta);
}
```

```{r}
invgam_mom(mean = mean(rnorm(1000, 0.5, 0.5)^2), 
           sd = sd(rnorm(1000, 0.5, 0.5)^2))
hist(1 / sqrt(rgamma(10000, shape = 2.69, rate = 0.85)))

ns <- 4
nt <- 300
musd <- 0.5
muhat <- rnorm(ns, 3, 1)
mu <- rnorm(ns, muhat, musd)
alpha <- 2.69
beta <- 0.85
prec <- rgamma(ns, shape = alpha, rate = beta)

testdata <- within(list(
  ns = ns, 
  nt = nt,
  A = setNames(map2(mu, prec, ~rlnorm(nt, .x, 1 / sqrt(.y))), 1:ns),
  muhat = muhat,
  musd = musd,
  alpha = alpha,
  beta = beta
  ), {
    A0 = sapply(A, min)
    dA = t(as.data.frame(A, optional = TRUE)) - 
      matrix(rep(A0, nt), nrow = ns, byrow = FALSE)
  })
post_stan <- sampling(testcase3, data = testdata, cores = 3, chains = 3,
                      control = list(adapt_delta = 0.99))
```

```{r}
pairs(post_stan, pars = "A0")
stan_trace(post_stan, inc_warmup = TRUE, pars = "lp__")
testdata$A0
stan_trace(post_stan, inc_warmup = TRUE, pars = "A0")
stan_trace(post_stan, inc_warmup = TRUE, pars = "prec")

stan_trace(post_stan, inc_warmup = FALSE)
stan_trace(post_stan, inc_warmup = FALSE, pars = "sigma")

stan_hist(post_stan, pars = "A0")


stan_hist(post_stan)
summary(post_stan)$summary
```


Can I modify testdata to use estA0?

```{r}
td_sl <- within(testdata, {
  W <- matrix(30, nr = nrow(dA), nc = ncol(dA))
  S <- matrix(0.001, nr = nrow(dA), nc = ncol(dA))
})

foo <- estA0_lm(td_sl)
summary(foo)
foo$model

pairs(foo$model)

bar <- batman_log(td_sl, iterlim = 1000)
bar$code
bar$A0
```

Not well defined! Matrix is rank deficient and max likelihood doesn't converge.

- Seems I need time-varying ws matrix.

## Truncated lognormal-normal model with additional obs

Since the last one was ill-defined (easy to see if you write out the math in the zero-error case), I'll add SWOT-like observations to supplement the dataset. This is getting really close to BAM. RHS can vary in time (but not space). LHS needs additional time- and space-varying observations. Unclear whether $\sigma$ can vary in space.

$$
(A_{0,s} + \delta A_{st})x_{st} \sim logN(\mu_t, \sigma_i) \\
\mu_s \sim N(\hat{\mu_s}, \sigma_\mu) \\
\sigma^2_s \sim IG(\alpha, \beta)
$$

If this were SWOT data, we could define $x_{st} = W^{-2/5}S^{3/10}

```{stan output.var = "testcase4"}
data {
  int ns;
  int nt;
  vector[nt] dA[ns];
  vector<lower=0>[nt] x[ns];
  real muhat;
  real musd;
  real alpha;
  real beta;
}

parameters {
  real mu;
  vector<lower=0>[ns] A0;
  real<lower=0.01> prec;
}

transformed parameters {
  //vector[nx] A[2]; 
  real<lower=0> sigma;
  
  sigma = sqrt(1 / prec);
}

model {
  // x ~ normal(mu, sigma);
  
  for (i in 1:ns) {
    (A0[i] + dA[i]) .* x[i] ~ lognormal(mu, sigma);
  }
  
  mu ~ normal(muhat, musd);
  prec ~ gamma(alpha, beta);
}
```

Now the tricky part: simulate the appropriate data. 

```{r}
# invgam_mom(mean = mean(rnorm(1000, 0.1, 0.2)^2), 
#            sd = sd(rnorm(1000, 0.05, 0.2)^2))
# hist(1 / sqrt(rgamma(10000, shape = 2.81, rate = 0.095)))

ns <- 4
nt <- 600
musd <- 0.5
muhat <- rnorm(1, 3, 1)
mu <- rnorm(1, muhat, musd)
alpha <- 2.81
beta <- 0.095
prec <- rgamma(1, shape = alpha, rate = beta)
sigma <- 1 / sqrt(prec)

mu_A <- rnorm(ns, 4.5, 1)
sigma_A <- abs(rnorm(ns, 0.5, 0.2))
A <- map2(mu_A, sigma_A, ~rlnorm(nt, .x, .y)) %>% 
  setNames(1:ns) %>% 
  as.data.frame() %>% 
  t()

lhs <- matrix(rlnorm(ns * nt, mu, sigma), nrow = ns, ncol = nt)
x <- lhs / A

testdata <- within(list(
  ns = ns, 
  nt = nt,
  A = A,
  x = x,
  muhat = muhat,
  musd = musd,
  alpha = alpha,
  beta = beta
  ), {
    A0 = apply(A, 1, min)
    dA = A - 
      matrix(rep(A0, nt), nrow = ns, byrow = FALSE)
  })
post_stan <- sampling(testcase4, data = testdata, cores = 3, chains = 3,
                      control = list(adapt_delta = 0.99))
```


```{r}
pairs(post_stan, pars = "A0")
stan_trace(post_stan, inc_warmup = TRUE, pars = "lp__")
testdata$A0
stan_trace(post_stan, inc_warmup = TRUE, pars = "A0")
stan_trace(post_stan, inc_warmup = TRUE, pars = "prec")

stan_trace(post_stan, inc_warmup = FALSE, pars = "sigma")
sigma

stan_trace(post_stan, inc_warmup = FALSE, pars = "prec")
prec

stan_trace(post_stan, inc_warmup = FALSE, pars = "mu")
mu


stan_hist(post_stan)
summary(post_stan)$summary
```

Interesting. Mu and A0 are biased low; sigma is biased a little high (meaning prec is biased low). Anyway, it's a consistent result (based on a single dataset). Can I replicate using gibbs?

```{r}


# Modified function from lib/gibbs.R

sample_A0 <- function(inputs, state) {
  
  # qnmat <- swot_vec2mat(state$qn, inputs$ws)
  logxvec <- as.vector(inputs$logx)
  
  minmat <- log(inputs$dA)
  minvec <- as.vector(minmat)
  
  avec <- truncnorm::rtruncnorm(inputs$ns * inputs$nt, 
                                  a = minvec, b = Inf,
                                  mean = state$mu - logxvec, 
                                  sd = state$sigma)
  amat <- matrix(avec, nrow = inputs$ns)
  A0_med <- exp(amat) - inputs$dA_med
  
  out <- apply(A0_med + inputs$dA_med - inputs$dA, 1, geomMean)
  
  out
}

testdata$logx <- log(testdata$x)
testdata$dA_med <- rezero_dA(testdata$dA, zero = "median")

# Initialize chain
niter <- 3000
precchain <- sigmachain <- muchain <- numeric(niter)
A0chain <- matrix(0, nrow = niter, ncol = ns)
inits <- list(
  mu = 0, 
  prec = 0.01,
  A0 = rep(0.1, ns))
inits$logA <- log(testdata$dA + swot_vec2mat(inits$A0, testdata$dA))
inits$sigma <- 1 / sqrt(inits$prec)

state <- inits
for (i in 1:niter) {
  cat(".")
  if (i %% 50 == 0) 
    cat(i, "\n")
  likmu <- mean(testdata$logx + state$logA)
  state$mu <- sample_normal(priormu = testdata$muhat, priorsigsq = (testdata$musd)^2, 
                            likmu = likmu, liksigsq = 1 / state$prec, 
                            likn = testdata$ns * testdata$nt)
  muchain[i] <- state$mu
  state$prec <- rgamma(1, shape = postalpha(prialpha = testdata$alpha, 
                                            n = testdata$ns * testdata$nt),
                       rate = postbeta(pribeta = testdata$beta, 
                                       y = as.vector(testdata$logx + state$logA), 
                                       mu = state$mu))
  state$sigma <- 1 / sqrt(state$prec)
  precchain[i] <- state$prec
  sigmachain[i] <- state$sigma
  
  state$A0 <- sample_A0(inputs = testdata, state = state)
  A0chain[i, ] <- state$A0
  state$logA <- log(swot_vec2mat(state$A0, testdata$dA) + testdata$dA)
}

plot(muchain)
plot(sigmachain)
plot(A0chain[, 1])
```
 
It appears the gibbs sampling of A0 does not work. Need recourse to a metropolis-in-gibbs. 